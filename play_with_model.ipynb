{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/khamies//LSTM-Sequence-VAE/blob/master/play_with_model.ipynb\" \n",
        "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LSTM-Variational-AutoEncoder'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Total 165 (delta 0), reused 0 (delta 0), pack-reused 165\u001b[K\n",
            "Receiving objects: 100% (165/165), 29.96 MiB | 20.70 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Download data and files.\n",
        "!git clone https://github.com/Khamies/LSTM-Variational-AutoEncoder.git\n",
        "import os \n",
        "os.chdir(\"LSTM-Variational-AutoEncoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bsX47UMWT0NT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from VGDLData.ptb import PTB\n",
        "from model import LSTM_VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u6QyBIaJWGLI"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "\n",
        "torch.manual_seed(1000)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "\n",
        "batch_size = 32\n",
        "bptt = 60\n",
        "lr = 0.001\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 256\n",
        "latent_size = 16\n",
        "lstm_layer=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcXo1SRDfIVm"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "k2uVseOtT_4J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN preprocessed file not found at VGDLData/ptb.train.json. Creating new.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'VGDLData/ptb.train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[39m=\u001b[39m PTB(data_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVGDLData\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, create_data\u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, max_sequence_length\u001b[39m=\u001b[39;49m bptt)\n\u001b[1;32m      3\u001b[0m test_data \u001b[39m=\u001b[39m PTB(data_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVGDLData\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, create_data\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, max_sequence_length\u001b[39m=\u001b[39mbptt)\n\u001b[1;32m      4\u001b[0m valid_data \u001b[39m=\u001b[39m PTB(data_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVGDLData\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, create_data\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, max_sequence_length\u001b[39m=\u001b[39m bptt)\n",
            "File \u001b[0;32m~/Desktop/Folder/Programming/Research/Sequence-VAE/VGDLData/ptb.py:33\u001b[0m, in \u001b[0;36mPTB.__init__\u001b[0;34m(self, data_dir, split, create_data, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_file)):\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m preprocessed file not found at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Creating new.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(split\u001b[39m.\u001b[39mupper(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_file)))\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_data()\n\u001b[1;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_data()\n",
            "File \u001b[0;32m~/Desktop/Folder/Programming/Research/Sequence-VAE/VGDLData/ptb.py:133\u001b[0m, in \u001b[0;36mPTB._create_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_vocab()\n\u001b[1;32m    134\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_vocab()\n",
            "File \u001b[0;32m~/Desktop/Folder/Programming/Research/Sequence-VAE/VGDLData/ptb.py:183\u001b[0m, in \u001b[0;36mPTB._create_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     i2w[\u001b[39mlen\u001b[39m(w2i)] \u001b[39m=\u001b[39m st\n\u001b[1;32m    181\u001b[0m     w2i[st] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(w2i)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mfor\u001b[39;00m i, words \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_words()):\n\u001b[1;32m    184\u001b[0m     w2c\u001b[39m.\u001b[39mupdate(words)\n\u001b[1;32m    186\u001b[0m \u001b[39mfor\u001b[39;00m w, c \u001b[39min\u001b[39;00m w2c\u001b[39m.\u001b[39mitems():\n",
            "File \u001b[0;32m~/Desktop/Folder/Programming/Research/Sequence-VAE/VGDLData/ptb.py:124\u001b[0m, in \u001b[0;36mPTB._get_words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_words\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    122\u001b[0m     words \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_data_path) \u001b[39mas\u001b[39;00m csv_file:\n\u001b[1;32m    125\u001b[0m         csv_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(csv_file, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    126\u001b[0m         \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m csv_reader:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VGDLData/ptb.train.csv'"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train_data = PTB(data_dir=\"./VGDLData\", split=\"train\", create_data= False, max_sequence_length= bptt)\n",
        "test_data = PTB(data_dir=\"./VGDLData\", split=\"test\", create_data= False, max_sequence_length=bptt)\n",
        "valid_data = PTB(data_dir=\"./VGDLData\", split=\"valid\", create_data= False, max_sequence_length= bptt)\n",
        "\n",
        "\n",
        "# Batchify the data\n",
        "train_loader = torch.utils.data.DataLoader( dataset= train_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "test_loader = torch.utils.data.DataLoader( dataset= test_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "valid_loader = torch.utils.data.DataLoader( dataset= valid_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-waTAMaZUIY9"
      },
      "outputs": [],
      "source": [
        "def interpolate(model, n_interpolations, sos, sequence_length):\n",
        "\n",
        "  # # Get input.\n",
        "\n",
        "  z1 = torch.randn((1,1,latent_size)).to(device)\n",
        "  z2 = torch.randn((1,1,latent_size)).to(device)\n",
        "\n",
        "  text1 = model.inference(sequence_length , sos, z1)\n",
        "  text2 = model.inference(sequence_length , sos, z2)\n",
        "\n",
        "  alpha_s = torch.linspace(0,1,n_interpolations)\n",
        "\n",
        "  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])\n",
        "\n",
        "\n",
        "  samples = [model.inference(sequence_length , sos, z) for z in interpolations]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return samples, text1, text2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ2jOq3se8-x"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw2_Fc_9Vhh5",
        "outputId": "0ffd9219-3097-4140-f899-b1d982c063d0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/VGDL_VAE.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab_size \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m LSTM_VAE(vocab_size \u001b[39m=\u001b[39m vocab_size, embed_size \u001b[39m=\u001b[39m embed_size, hidden_size \u001b[39m=\u001b[39m hidden_size, latent_size \u001b[39m=\u001b[39m latent_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/VGDL_VAE.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m) )\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/VGDL_VAE.pt'"
          ]
        }
      ],
      "source": [
        "vocab_size = train_data.vocab_size\n",
        "model = LSTM_VAE(vocab_size = vocab_size, embed_size = embed_size, hidden_size = hidden_size, latent_size = latent_size).to(device)\n",
        "\n",
        "checkpoint = torch.load(\"models/LSTM_VAE.pt\", map_location=torch.device('cpu') )\n",
        "model.load_state_dict(checkpoint[\"model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKhUHL_7Vg5X",
        "outputId": "cbd2db6b-67ab-4678-b0ea-4cfdc3f56069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the company said it will close the <unk> offering for\n",
            "the company said it will close the <unk> offering for\n"
          ]
        }
      ],
      "source": [
        "#@title Sample Generation\n",
        "# inference\n",
        "z1 = torch.randn(1,1,latent_size).to(device)\n",
        "z2 = torch.randn(1,1,latent_size).to(device)\n",
        "\n",
        "sos = \"<sos>\"\n",
        "sample1 = model.inference(10 , sos, z1)\n",
        "sample2 = model.inference(10 , sos , z2)\n",
        "\n",
        "\n",
        "print(sample1)\n",
        "print(sample2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eme4eXbjVPOM",
        "outputId": "0db5e4c4-3e49-452c-cd97-514519133f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First sentence: bush veto power would require the fed to get a\n",
            "Second sentence: bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n"
          ]
        }
      ],
      "source": [
        "#@title Interpolation\n",
        "samples, text1, text2 = interpolate(model, 20,\"president\", 10)\n",
        "print(\"First sentence:\", text1)\n",
        "print(\"Second sentence:\", text2)\n",
        "\n",
        "for sample in samples: print(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Play_lstm_seq_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
